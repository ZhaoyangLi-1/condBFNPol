#!/usr/bin/env python
"""
Statistical Analysis: BFN vs Diffusion Policy
For Google Research Paper

This script performs statistical tests to demonstrate significance of results.

Usage:
    python scripts/statistical_analysis.py
"""

import numpy as np
from pathlib import Path
import json
from typing import Dict, List
from scipy import stats
import argparse

def extract_scores_from_logs(base_dir: str = "cluster_checkpoints/benchmarkresults") -> Dict:
    """Extract test scores from logs."""
    base_path = Path(base_dir)
    results = {'bfn': [], 'diffusion': []}
    
    for date_dir in base_path.iterdir():
        if not date_dir.is_dir():
            continue
        
        for run_dir in date_dir.iterdir():
            if not run_dir.is_dir():
                continue
            
            dir_name = run_dir.name
            method = None
            if "bfn" in dir_name.lower():
                method = "bfn"
            elif "diffusion" in dir_name.lower():
                method = "diffusion"
            else:
                continue
            
            log_file = run_dir / "logs.json.txt"
            if not log_file.exists():
                continue
            
            # Get max score from logs
            scores = []
            with open(log_file) as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        if 'test/mean_score' in entry:
                            scores.append(entry['test/mean_score'])
                    except:
                        pass
            
            if scores:
                results[method].append(max(scores))
    
    return results


def perform_statistical_tests(bfn_scores: List[float], diff_scores: List[float]):
    """Perform comprehensive statistical tests."""
    
    bfn_scores = np.array(bfn_scores)
    diff_scores = np.array(diff_scores)
    
    print("\n" + "="*70)
    print("STATISTICAL ANALYSIS: BFN vs Diffusion Policy")
    print("="*70)
    
    # Descriptive statistics
    print("\nðŸ“Š DESCRIPTIVE STATISTICS")
    print("-"*70)
    print(f"BFN Policy:")
    print(f"  N = {len(bfn_scores)}")
    print(f"  Mean = {np.mean(bfn_scores):.4f}")
    print(f"  Std = {np.std(bfn_scores):.4f}")
    print(f"  Median = {np.median(bfn_scores):.4f}")
    print(f"  Min = {np.min(bfn_scores):.4f}")
    print(f"  Max = {np.max(bfn_scores):.4f}")
    
    print(f"\nDiffusion Policy:")
    print(f"  N = {len(diff_scores)}")
    print(f"  Mean = {np.mean(diff_scores):.4f}")
    print(f"  Std = {np.std(diff_scores):.4f}")
    print(f"  Median = {np.median(diff_scores):.4f}")
    print(f"  Min = {np.min(diff_scores):.4f}")
    print(f"  Max = {np.max(diff_scores):.4f}")
    
    # Normality tests
    print("\nðŸ“ˆ NORMALITY TESTS")
    print("-"*70)
    bfn_shapiro = stats.shapiro(bfn_scores) if len(bfn_scores) <= 50 else None
    diff_shapiro = stats.shapiro(diff_scores) if len(diff_scores) <= 50 else None
    
    if bfn_shapiro:
        print(f"BFN Shapiro-Wilk: W={bfn_shapiro.statistic:.4f}, p={bfn_shapiro.pvalue:.4f}")
        print(f"  {'Normally distributed' if bfn_shapiro.pvalue > 0.05 else 'NOT normally distributed'}")
    else:
        print("BFN: Too many samples for Shapiro-Wilk (using D'Agostino test)")
        bfn_agostino = stats.normaltest(bfn_scores)
        print(f"  D'Agostino: statistic={bfn_agostino.statistic:.4f}, p={bfn_agostino.pvalue:.4f}")
    
    if diff_shapiro:
        print(f"Diffusion Shapiro-Wilk: W={diff_shapiro.statistic:.4f}, p={diff_shapiro.pvalue:.4f}")
        print(f"  {'Normally distributed' if diff_shapiro.pvalue > 0.05 else 'NOT normally distributed'}")
    else:
        print("Diffusion: Too many samples for Shapiro-Wilk")
        diff_agostino = stats.normaltest(diff_scores)
        print(f"  D'Agostino: statistic={diff_agostino.statistic:.4f}, p={diff_agostino.pvalue:.4f}")
    
    # Parametric test: T-test
    print("\nðŸ”¬ PARAMETRIC TESTS")
    print("-"*70)
    
    # Independent samples t-test
    t_stat, t_pvalue = stats.ttest_ind(bfn_scores, diff_scores)
    print(f"Independent Samples T-test:")
    print(f"  t-statistic = {t_stat:.4f}")
    print(f"  p-value = {t_pvalue:.4f}")
    print(f"  {'Significant difference' if t_pvalue < 0.05 else 'No significant difference'} (Î±=0.05)")
    
    # Welch's t-test (doesn't assume equal variances)
    welch_stat, welch_pvalue = stats.ttest_ind(bfn_scores, diff_scores, equal_var=False)
    print(f"\nWelch's T-test (unequal variances):")
    print(f"  t-statistic = {welch_stat:.4f}")
    print(f"  p-value = {welch_pvalue:.4f}")
    print(f"  {'Significant difference' if welch_pvalue < 0.05 else 'No significant difference'} (Î±=0.05)")
    
    # Non-parametric test: Mann-Whitney U
    print("\nðŸ”¬ NON-PARAMETRIC TESTS")
    print("-"*70)
    u_stat, u_pvalue = stats.mannwhitneyu(bfn_scores, diff_scores, alternative='two-sided')
    print(f"Mann-Whitney U test:")
    print(f"  U-statistic = {u_stat:.4f}")
    print(f"  p-value = {u_pvalue:.4f}")
    print(f"  {'Significant difference' if u_pvalue < 0.05 else 'No significant difference'} (Î±=0.05)")
    
    # Effect sizes
    print("\nðŸ“ EFFECT SIZES")
    print("-"*70)
    
    # Cohen's d
    pooled_std = np.sqrt(((len(bfn_scores) - 1) * np.var(bfn_scores) + 
                          (len(diff_scores) - 1) * np.var(diff_scores)) / 
                         (len(bfn_scores) + len(diff_scores) - 2))
    cohens_d = (np.mean(bfn_scores) - np.mean(diff_scores)) / pooled_std
    print(f"Cohen's d = {cohens_d:.4f}")
    if abs(cohens_d) < 0.2:
        effect_size = "negligible"
    elif abs(cohens_d) < 0.5:
        effect_size = "small"
    elif abs(cohens_d) < 0.8:
        effect_size = "medium"
    else:
        effect_size = "large"
    print(f"  Effect size: {effect_size}")
    
    # Confidence intervals
    print("\nðŸ“Š CONFIDENCE INTERVALS (95%)")
    print("-"*70)
    
    bfn_se = stats.sem(bfn_scores)
    bfn_ci = stats.t.interval(0.95, len(bfn_scores)-1, loc=np.mean(bfn_scores), scale=bfn_se)
    print(f"BFN: [{bfn_ci[0]:.4f}, {bfn_ci[1]:.4f}]")
    
    diff_se = stats.sem(diff_scores)
    diff_ci = stats.t.interval(0.95, len(diff_scores)-1, loc=np.mean(diff_scores), scale=diff_se)
    print(f"Diffusion: [{diff_ci[0]:.4f}, {diff_ci[1]:.4f}]")
    
    # Difference of means CI
    diff_mean = np.mean(diff_scores) - np.mean(bfn_scores)
    diff_se_combined = np.sqrt(bfn_se**2 + diff_se**2)
    diff_ci = stats.t.interval(0.95, min(len(bfn_scores), len(diff_scores))-1, 
                               loc=diff_mean, scale=diff_se_combined)
    print(f"\nDifference (Diffusion - BFN): [{diff_ci[0]:.4f}, {diff_ci[1]:.4f}]")
    
    # Efficiency analysis
    print("\nâš¡ EFFICIENCY ANALYSIS")
    print("-"*70)
    
    # Assuming 20 steps for BFN, 100 for Diffusion
    bfn_steps = 20
    diff_steps = 100
    
    bfn_efficiency = np.mean(bfn_scores) / bfn_steps
    diff_efficiency = np.mean(diff_scores) / diff_steps
    
    print(f"BFN Efficiency (score/step): {bfn_efficiency:.6f}")
    print(f"Diffusion Efficiency (score/step): {diff_efficiency:.6f}")
    print(f"Efficiency Ratio (BFN/Diffusion): {bfn_efficiency/diff_efficiency:.2f}Ã—")
    
    # Statistical test on efficiency
    bfn_eff_scores = bfn_scores / bfn_steps
    diff_eff_scores = diff_scores / diff_steps
    
    eff_u_stat, eff_u_pvalue = stats.mannwhitneyu(bfn_eff_scores, diff_eff_scores, alternative='two-sided')
    print(f"\nMann-Whitney U test on efficiency:")
    print(f"  p-value = {eff_u_pvalue:.4f}")
    print(f"  {'BFN significantly more efficient' if eff_u_pvalue < 0.05 else 'No significant efficiency difference'}")
    
    return {
        'bfn_scores': bfn_scores.tolist(),
        'diff_scores': diff_scores.tolist(),
        't_test': {'statistic': float(t_stat), 'pvalue': float(t_pvalue)},
        'welch_test': {'statistic': float(welch_stat), 'pvalue': float(welch_pvalue)},
        'mannwhitney': {'statistic': float(u_stat), 'pvalue': float(u_pvalue)},
        'cohens_d': float(cohens_d),
        'efficiency_ratio': float(bfn_efficiency/diff_efficiency),
        'efficiency_pvalue': float(eff_u_pvalue),
    }


def generate_latex_statistics(results: Dict, output_file: str = "figures/tables/table_statistical_analysis.tex"):
    """Generate LaTeX table with statistical results."""
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    bfn_scores = results['bfn_scores']
    diff_scores = results['diff_scores']
    
    bfn_mean = np.mean(bfn_scores)
    bfn_std = np.std(bfn_scores)
    diff_mean = np.mean(diff_scores)
    diff_std = np.std(diff_scores)
    
    # Compute CIs
    bfn_se = stats.sem(bfn_scores)
    bfn_ci = stats.t.interval(0.95, len(bfn_scores)-1, loc=bfn_mean, scale=bfn_se)
    diff_se = stats.sem(diff_scores)
    diff_ci = stats.t.interval(0.95, len(diff_scores)-1, loc=diff_mean, scale=diff_se)
    
    significance_text = "significant" if results['mannwhitney']['pvalue'] < 0.05 else "not statistically significant"
    eff_significance = "statistically significant" if results['efficiency_pvalue'] < 0.05 else "not statistically significant"
    
    table = """%% =============================================================================
%% TABLE: Statistical Analysis - BFN vs Diffusion Policy
%% =============================================================================

\\begin{table}[t]
\\centering
\\caption{\\textbf{Statistical Analysis of BFN vs Diffusion Policy.}
Comparison of performance scores with statistical tests. While Diffusion Policy 
achieves slightly higher mean performance, the difference is %s
(p = %.4f, Mann-Whitney U test). BFN's efficiency advantage (%.1f\\texttimes{} better score per step) 
is %s (p = %.4f).}
\\label{tab:statistical_analysis}
\\vspace{0.5em}
\\small
\\begin{tabular}{@{}lcccc@{}}
\\toprule
\\textbf{Method} & \\textbf{Mean $\\pm$ Std} & \\textbf{95\\%% CI} & \\textbf{N} & \\textbf{Max} \\\\
\\midrule
BFN Policy & $%.3f \\pm %.3f$ & [%.3f, %.3f] & %d & %.3f \\\\
Diffusion Policy & $%.3f \\pm %.3f$ & [%.3f, %.3f] & %d & %.3f \\\\
\\midrule
\\textbf{Difference} & $%.4f$ & & & \\\\
\\textbf{Cohen's d} & $%.3f$ & \\textit{%s effect} & & \\\\
\\textbf{p-value} & $%.4f$ & (Mann-Whitney U) & & \\\\
\\bottomrule
\\end{tabular}
\\vspace{0.3em}
\\raggedright
\\footnotesize
\\textit{Note:} CI = Confidence Interval. Effect size interpretation: 
|d| < 0.2 = negligible, 0.2-0.5 = small, 0.5-0.8 = medium, > 0.8 = large.
\\end{table}
""" % (
        significance_text,
        results['mannwhitney']['pvalue'],
        results['efficiency_ratio'],
        eff_significance,
        results['efficiency_pvalue'],
        bfn_mean, bfn_std, bfn_ci[0], bfn_ci[1], len(bfn_scores), np.max(bfn_scores),
        diff_mean, diff_std, diff_ci[0], diff_ci[1], len(diff_scores), np.max(diff_scores),
        diff_mean - bfn_mean,
        results['cohens_d'],
        "large" if abs(results['cohens_d']) > 0.8 else ("medium" if abs(results['cohens_d']) > 0.5 else ("small" if abs(results['cohens_d']) > 0.2 else "negligible")),
        results['mannwhitney']['pvalue'],
    )
    
    with open(output_path, 'w') as f:
        f.write(table)
    
    print(f"\nâœ“ Statistical analysis table saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Statistical analysis of BFN vs Diffusion")
    parser.add_argument("--checkpoint-dir", type=str,
                        default="cluster_checkpoints/benchmarkresults",
                        help="Directory containing training logs")
    parser.add_argument("--output", type=str,
                        default="figures/tables/table_statistical_analysis.tex",
                        help="Output LaTeX table file")
    
    args = parser.parse_args()
    
    # Extract scores
    print("Extracting scores from training logs...")
    results_dict = extract_scores_from_logs(args.checkpoint_dir)
    
    bfn_scores = results_dict['bfn']
    diff_scores = results_dict['diffusion']
    
    if not bfn_scores or not diff_scores:
        print("Error: Need scores from both methods")
        return
    
    # Perform statistical tests
    stats_results = perform_statistical_tests(bfn_scores, diff_scores)
    
    # Add raw scores to results for LaTeX
    stats_results['bfn_scores'] = bfn_scores
    stats_results['diff_scores'] = diff_scores
    
    # Generate LaTeX table
    generate_latex_statistics(stats_results, args.output)
    
    # Summary
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    print(f"BFN achieves {np.mean(bfn_scores):.3f} Â± {np.std(bfn_scores):.3f}")
    print(f"Diffusion achieves {np.mean(diff_scores):.3f} Â± {np.std(diff_scores):.3f}")
    print(f"\nEfficiency advantage: {stats_results['efficiency_ratio']:.2f}Ã— (BFN/Diffusion)")
    print(f"Statistical significance: p = {stats_results['efficiency_pvalue']:.4f}")


if __name__ == "__main__":
    main()
